Before you start make sure to read: 



1)https://docs.google.com/document/d/1PeWefW2toThs-Pbw0CMv2us7wxQI0gRrP1LGuwMp_UQ/edit

2)https://docs.google.com/document/d/1qpuNCmBmu4KcsS_hE2srewcCiP4f9P5cCyDfHmsSAVU/edit





# RESEARCH(Green)

First pick a link in the spreadsheet to evaluate. 

Fill in your name in Column __ to claim your link so there are no duplicate researching efforts. 

Keep in mind two things when looking at the spreadsheet: 



### <u>**Is the data actually actually crawlable?**</u>

****

- Yes, if is crawlable, nominate it:  go to chrome extension (__link), if that doesn't work then boomarklet for seeding (link)
  - Fill out that you seeded (Column __)
  - Fill out that link is done (Column __)
- Kind of crawlable (FTP, mixed content, big data sets): follow steps for yes and no
  - *While we understand that this may mean duplicate sets of data in the ckan, that is not a concern. We are ensuring that the data is fully preserved and accessible. 
- No, search agency websites and data.gov for dataset entry points for your dataset collection   
  - Add harvastable data url to spreadsheet (Column ), REALLY IMPORTANT! 
  - Learn what actual datasets look like in terms of format  (SQL, FTP, ZIP, PDF Collections, etc.), size,  what you found, etc. (Column )

***<u>Keep in mind Look for related rows and ensure that you aren't harvesting a sub directory if you can harvest the entire directory.</u>*****



# HARVESTING(Purple)

Check out a URL on a google sheet 

Link identification - method of identification 

List of URLS 

WGet loop —> consumes list with 4 second delay between each download so we don't get kicked out  —> data directory 

Write metadata. Json with as much info as possible

Package as [uuid.zip] and [uuid.json] 

Mark as Harvested google sheet

# BAGGING(Red)

- Check out a package (Column )
- Data comes from thumbdrive
- Take folder and create metadata folder for Json template. 
- Label the json file and the folder where the json folder where it is located with the uuid.
- The json should 

```
{
  "Individual source or seed URL": "http://www.eia.gov/renewable/data.cfm",
  "UUID" : "E30FA3CA-C5CB-41D5-8608-0650D1B6F105",
  "id_agency" : 2,
  "id_subagency": ,
  "id_org":,
  "id_suborg":,
  "Institution facilitating the data capture creation and packaging": "Penn Data Refuge",
  "Date of capture": "2017-01-17",
  "Federal agency data acquired from": "Department of Energy/U.S. Energy Information Administration",
  "Name of resource": "Renewable and Alternative Fuels",
  "File formats contained in package": ".pdf, .zip",
  "Type(s) of content in package": "datasets, codebooks",
  "Free text description of capture process": "Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.",
  "Name of package creator": "Mallick Hossain and Ben Goldman"
  }
```

- Make sure to save as a .json file.

- Copy the metadata file into folder where the package is 

- Run python command line script which creates the bag 

  - [Python script to make a bag (command line)]: https://github.com/LibraryOfCongress/bagit-python

    ****

  ```
  bagit.py --contact-name 'John Kunze' /directory/to/bag
  ```

- You should be left with a metadata file and four seperate bagit files 

  - bag-info.txt
  - bagit.txt
  - manifest-md5.txt
  - tagmanifest-md5.txt
  - data  (package of stuff)

- Zip this entire bagit file, data file, plus four txt files listed 

- Put the zip file onto a thumb drive and give to an uploader

# UPLOADING(Yellow)

- Claim you the link you are uploading (Column)
- Follow the instructions below:

You will need python and pip in order for this to work.

#### **Getting your machine ready to upload:**

**Download:**

```
pip install awscli

```

Get your access keys from the system administrator for the s3 bucket. You should have two access keys, an "Access key ID" and a "Secret access key"

**Run:**

```
aws configure

```

The script will ask for a few things:

```
AWS Access Key ID [None]: 
AWS Secret Access Key [None]: 
Default region name [None]: 
Default output format [None]: 

```

Enter your two aws access keys

Press enter for the default output format and region name (you don't need to have anything in this field)

**Run:**

`aws s3 ls ckanholding`

You should see a list of files, such as:

```
2017-01-12 16:38:44  713031680 CentOS-7-x86_64-Minimal-1611.iso
2017-01-12 18:35:20     104065 Linear_Bone_13.stl
2017-01-12 16:25:00         21 index.html

```

------

### **Uploading files**

**To upload a file or folder to the s3 bucket:**

```
aws s3 cp --recursive [folder name]  s3://ckanholding

aws s3 cp [file name]  s3://ckanholding
```

# METADATA(Blue)

- Get thumbdrive from uploader 

- Checkout metadata link for file (column)

- Create new record in CKan

- Follow metadata schema spreadsheet (link)

- Go to original url (Column) to gather the rest of the metadata facts needed to populate the fields in the metadata mapping form

- Upload the JSON file (from thumbdrive) to CKan

- Click link from the S3 bucket that you have populated in the form that you copied from the spreadsheet. 

- The link should download the bag file, but we want to double check.

- Unzip the file

- Spot check the files (Column )

  - If there are problems talk to a guid

- Let everyone know you are done (Column )

  ​
