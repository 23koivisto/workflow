# HARVESTING 

**Check out a URL**

**THIS WOULD BE A GREAT PLACE FOR SOMEONE TO PROVIDE LINKS AND INFO TO THE VARIOUS REPOS AND DOCS ALL OVER THAT DESCRIBE THIS PROCESS. I FEEL LIKE I'VE LOST TRACK**

* get the data
 * Link identification - method of identification 
 * List of URLS 
 * WGet loop —> consumes list with 4 second delay between each download so we don't get kicked out  —> data directory 

* Write metadata. Json with as much info as possible. **ANOTER PLACE THAT COULD USE MORE INFO**

* Package as [uuid.zip] and [uuid.json] 

* **Upload to staging area (needs more info from b5, or link to someone who can give you an invite in)**
* **Check in data to signal to baggers that the data is there**

